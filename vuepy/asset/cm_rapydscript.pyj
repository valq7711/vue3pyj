MODE_NAME = 'rapydscript'
CodeMirror = None

def reg_mode(CM):
    nonlocal CodeMirror
    CodeMirror = CM
    CM.defineMode(MODE_NAME, def_mode)

def arr2hash(arr):
    ret = {}
    arr.forEach(def(it): ret[it] = True;)
    return ret

def wordRegexp(words):
    return RegExp("^((" + words.join(")|(") + "))\\b")

def def_mode(conf, parserConf):

    ERRORCLASS = "error";
    #/[\[\]{}\(\),;\:\.]/
    punc = parserConf.punctuation or /^[\(\)\[\]\{\}@,:`;\.\\]/

    tripleOperators = parserConf.tripleOperators or /^((\/\/=)|(>>=)|(<<=)|(\*\*=))/
    doubleOperators = parserConf.doubleOperators or /^((==)|(!=)|(<=)|(>=)|(<>)|(<<)|(>>)|(\/\/)|(\*\*)|(\+=)|(\-=)|(\*=)|(%=)|(\/=)|(&=)|(\|=)|(\^=))/
    singleOperators = parserConf.singleOperators or /^[\+\-\*\/%&|\^~<>!=\?]/
    wordOperators = wordRegexp(["and", "or", "not", "is", "in", "del"])

    identifiers = parserConf.identifiers or RegExp("^[_A-Za-z\$][_A-Za-z0-9\$]*")

    indentUnit = conf.indentUnit
    hangingIndent = parserConf.hangingIndent or indentUnit

    stringPrefixes = RegExp("^(([rub]|(ur)|(br))?('{3}|\"{3}|['\"]))", "i")


    commonKeywords = ["as", "assert", "break", "class", "continue",
                    "def", "elif", "else", "except", "finally",
                    "for", "from", "global", "if", "import",
                    "lambda", "pass", "raise", "return",
                    "try", "while", "with", "yield", 'async']
    commonBuiltins = ["abs", "all", "any", "bin", "bool", "bytearray", "callable", "chr",
                    "classmethod", "compile", "complex", "delattr", "dict", "dir", "divmod",
                    "enumerate", "eval", "filter", "float", "format", "frozenset",
                    "getattr", "globals", "hasattr", "hash", "help", "hex", "id",
                    "input", "int", "isinstance", "issubclass", "iter", "len",
                    "list", "locals", "map", "max", "memoryview", "min", "next",
                    "object", "oct", "open", "ord", "pow", "property", "range",
                    "repr", "reversed", "round", "set", "setattr", "slice",
                    "sorted", "staticmethod", "str", "sum", "super", "tuple",
                    "type", "vars", "zip", "__import__", "NotImplemented",
                    "Ellipsis", "__debug__"]
    py2 = {builtins: ["apply", "basestring", "buffer", "cmp", "coerce", "execfile",
                    "file", "intern", "long", "raw_input", "reduce", "reload",
                    "unichr", "unicode", "xrange", "False", "True", "None"],
            keywords: ["exec", "print"]}
    py3 = {builtins: ["ascii", "bytes", "exec", "print"],
             keywords: ["nonlocal", "False", "True", "None"]}

    _keywords = commonKeywords.concat(py2.keywords)
    _builtins = commonBuiltins.concat(py2.builtins)

    _keywords = _keywords.concat(py3.keywords)
    _builtins = _builtins.concat(py3.builtins)

    if parserConf.extra_keywords:
        _keywords = _keywords.concat(parserConf.extra_keywords)

    if parserConf.extra_builtins:
        _builtins = _builtins.concat(parserConf.extra_builtins)

    keywords = arr2hash(_keywords)
    builtins = arr2hash(_builtins)

    CodeMirror.registerHelper("hintWords", MODE_NAME, _keywords.concat(_builtins))

    class RS_mode:
        def start_state(self, basecolumn):
            return {
                    tokenize: self.token_base,
                    scopes: [{offset: basecolumn or 0, type: "py", align: None, inline: None}],
                    lastStyle: None,
                    lastToken: None,
                    lastPunc: None,
                    dedent: 0,
                    indent: None,
                    is_glueLine: False,
                    block_expect: False
            }

        def load_state(self, state):
            Object.assign(self, state)

        def update_state(self, state):
            st_keys = Object.keys(self.start_state())
            st_keys.forEach(def(k):
                state[k] = self[k];
            )

        def dedent_process(self, stream):
            indented = stream.indentation()
            scopes = self.scopes
            while scopes[-1].offset > indented and scopes[-1].type is 'py':
                scopes.pop()
            return {error: scopes.type is 'py' and scopes[-1].offset != indented}

        def token_re(self, stream):
            escaped = False
            inSet = False
            while (next = stream.next()):
                if not escaped:
                    if next == "/" and not inSet:
                        return
                    if next == "[":
                        inSet = True
                    elif inSet and next == "]":
                        inSet = False
                escaped = not escaped and next == "\\"

        def  token_base(self, stream):
            # Handle scope changes
            sol = stream.sol() and not self.is_glueLine
            if sol:
                block_expect = self.block_expect
                self.block_expect = False
                indent = self.indent = stream.indentation()
                scope_offset = self.scopes[-1].offset
                line_offset = indent
                if line_offset > scope_offset and block_expect:
                    self.push_scope(stream, "py", line_offset)
                elif self.scopes[-1].type is 'py' and line_offset < scope_offset and self.dedent_process(stream).error:
                    stream.skipToEnd()
                    return ERRORCLASS
            return self.token_baseInner(stream)


        def token_baseInner(self, stream):
            if stream.eatSpace():
                return None

            ch = stream.peek()

            # Handle Comments
            if ch == "#":
                stream.skipToEnd()
                return "comment"

            # Handle Number Literals
            if stream.match(/^[0-9\.]/, False):
                # Floats
                if stream.match(/^\d*\.\d+(e[\+\-]?\d+)?/i) \
                        or stream.match(/^\d+\.\d*/) \
                        or stream.match(/^\.\d+/):
                    stream.eat(/J/i) # Float literals may be "imaginary"
                    return "number"

                # Integers
                if stream.match( /^0x[0-9a-f]+/i ) \
                        or stream.match(/^0b[01]+L?/i) \
                        or stream.match(/^0o[0-7]+L?/i) \
                        or stream.match(/^[0-9]\d*(e[\+\-]?\d+)?(L|J)?/i): # Decimal, (L|J) - Long or "imaginary"
                    return "number"

            # Handle Strings
            if stream.match(stringPrefixes):
                self.tokenize = self.token_string_factory(stream.current())
                return self.tokenize(stream)


            # Handle triple, double and word operators
            if stream.match(tripleOperators) or stream.match(doubleOperators) \
                    or stream.match(wordOperators):
                return "operator"

            # may be regexp
            if stream.eat('/'):
                if not self.lastToken or self.lastStyle == 'operator' \
                        or self.lastToken in ["return", 'if', 'elif', 'while', 'in'] \
                        or  /^[\[{\(:=,]$/.test(self.lastToken):
                    self.token_re(stream)
                    stream.match(/^\b(([gimyu])(?![gimyu]*\2))+\b/)
                    return "string-2"
                else:
                    return "operator"

            if stream.match(singleOperators):
                return "operator"

            if stream.match(punc):
                return "punctuation"

            if (w = stream.match(identifiers)):
                w = w[0]
                if keywords[w] or /^(get|set)$/.test(w) and stream.match(/ +[_A-Za-z$]/, False):
                    return "keyword"
                if builtins[w]:
                    return "builtin"
                if /^(self|cls)\b/.test(w):
                    return "variable-2"
                else:
                    addClass = w.startsWith('$') ? " variable-buck" : ''
                    return (/^(def|class|get|set)$/.test(self.lastToken) ? "def" : "variable") + addClass

            # Handle non-detected items
            stream.next()
            return ERRORCLASS


        def token_string_factory(self, delimiter):
            while "rub".indexOf(delimiter.charAt(0).toLowerCase()) >= 0:
                delimiter = delimiter.substr(1)

            singleline = delimiter.length == 1
            OUTCLASS = "string"

            def token_string(stream):
                while not stream.eol():
                    stream.eatWhile(/[^'"\\]/)
                    if stream.eat("\\"):
                        stream.next()
                        if singleline and stream.eol():
                            return OUTCLASS
                    elif stream.match(delimiter):
                        self.tokenize = self.token_base
                        return OUTCLASS
                    else:
                        stream.eat(/['"]/)

                if singleline:
                    if parserConf.singleLineStringErrors:
                        return ERRORCLASS
                    else:
                        self.tokenize = self.token_base
                return OUTCLASS

            token_string.isString = True
            return token_string

        def push_scope(self, stream, type, line_offset):
            offset = 0
            align = None
            inline = False
            # keep `def`-scope if only it is followed by `()`-scope
            if type is not ')' and self.scopes[-1].type is 'def':
                self.scopes.pop()
            if type == 'py':
                offset = line_offset
                self.block_expect = False
            elif type is 'def':
                tmp = self.scopes[-1]
                offset = tmp.offset or 0
                align = tmp.align
            else: # brakets
                if not stream.match(/^(\s|#.*)*$/, False):
                    offset = self.scopes[-1].offset
                    inline = True
                else:
                    offset = self.scopes[-1].offset + hangingIndent
            self.scopes.push({offset: offset, type: type, align: align, inline: inline})


        def token_lexer(self, stream):
            style = self.tokenize(stream)
            current = stream.current()
            self.is_glueLine = False
            eol = stream.eol()
            # Handle `def` scope
            if not eol:
                if current is "def" and self.scopes[-1].type is not 'py':
                    self.push_scope(stream, "def") # maybe start new `py` scope in brakets
                    return style
            # if eol and not '(' and top scope is still `def`
            # then there is no `()` after it
            elif current is not '(' and self.scopes[-1].type is 'def':
                self.scopes.pop()
                self.block_expect = False


            # Handle '.' connected identifiers
            if current == ".":
                style = stream.match(identifiers, False) ? None : ERRORCLASS
                if style == None and self.lastStyle == "meta":
                    # Apply 'meta' style to '.' connected identifiers when
                    # appropriate.
                    style = "meta"
                return style


            # Handle decorators
            if current == "@":
                return stream.match(identifiers, False) ? "metasym" : ERRORCLASS

            if (style == "variable" or style == "builtin") \
                    and self.lastStyle and self.lastStyle.startsWith("meta"):
                style = "meta"

            # Handle scope changes
            if self.scopes[-1].type == 'py' and ( current == "pass" or current == "return"):
                self.dedent += 1

            # Handle `:` scope effect changes
            if current is ":":
                if stream.match(/^(\s|#.*)*$/, False):
                    self.block_expect = self.block_expect or self.scopes[-1].type is 'py'

            # Handle brakets scope start/end
            else:
                delimiter_index = current.length == 1 ? "[({".indexOf(current) : -1
                if delimiter_index != -1:
                    self.push_scope(stream, "])}"[delimiter_index])
                else:
                    delimiter_index = "])}".indexOf(current)
                    if delimiter_index != -1:
                        if self.scopes[-1].type == current:
                            self.scopes.pop()
                            if current is ')'  and self.scopes[-1].type is 'def':
                                self.scopes.pop()
                                if not eol:
                                    self.block_expect = True
                        else:
                            return ERRORCLASS

            if self.dedent > 0 and eol and not self.is_glueLine and self.scopes[-1].type == "py":
                if self.scopes.length > 1:
                    self.scopes.pop()
                self.dedent -= 1

            # handle glue line
            if style == 'punctuation' and current == '\\':
                if  eol:
                    self.is_glueLine = True
                else:
                    stream.skipToEnd()
                    style = ERRORCLASS
            return style


    rs_mode = RS_mode()

    def start_state(basecol):
        return rs_mode.start_state(basecol)

    def token(stream, state):
        rs_mode.load_state(state)
        style = rs_mode.token_lexer(stream)
        if not rs_mode.is_glueLine:
            if style:
                rs_mode.lastStyle = style
            current = stream.current()
            if current and style:
                rs_mode.lastToken = current
            if stream.eol() and rs_mode.scopes[-1].type == ':':
                # `:` is missed
                rs_mode.scopes.pop()
                style = ERRORCLASS
        # is_glueLine == True also means EOL == True
        rs_mode.update_state(state)
        return style

    def indent(state, textAfter):
        rs_mode.load_state(state)
        if rs_mode.tokenize and rs_mode.tokenize != rs_mode.token_base:
            return rs_mode.tokenize.isString ? CodeMirror.Pass : 0
        scope = rs_mode.scopes[-1]
        closing = textAfter and textAfter.charAt(0) == scope.type
        if rs_mode.block_expect:
            if not closing:
                n = 1
                add_offset = indentUnit
            elif rs_mode.scopes.length > 1:
                n = 2
                add_offset = 0
            slen = rs_mode.scopes.length
            while n < slen and rs_mode.scopes[slen-n].inline:
                n += 1
            return rs_mode.scopes[slen-n].offset + add_offset
        #elif scope.align != None:
        #    return scope.align - (closing and state.lastToken != ',' ? 1 : 0)
        elif closing and rs_mode.scopes.length > 1:
            n = 2
            slen = rs_mode.scopes.length
            while n < slen and rs_mode.scopes[slen-n].inline:
                n += 1
            return rs_mode.scopes[slen-n].offset
        else:
            return scope.inline ? scope.offset + indentUnit : scope.offset

    external = {
        startState: start_state,
        token: token,
        indent: indent,
        lineComment: "#",
        fold: "indent",
        electricInput: /^\s*[\}\]\)]$/
    }
    return external

    #CodeMirror.defineMIME("text/x-python", "python")
    """
        words = def(str): return str.split(" ");


        CodeMirror.defineMIME("text/x-cython", {
            name: "python",
            extra_keywords: words("by cdef cimport cpdef ctypedef enum except"+
                              "extern gil include nogil property public"+
                              "readonly struct union DEF IF ELIF ELSE")

    """